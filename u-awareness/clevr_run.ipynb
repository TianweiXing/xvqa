{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from src.dataset import CLEVR, collate_data, transform, GQA\n",
    "from src.model import MACNetwork\n",
    "# from model_gqa import MACNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_epoch = 25\n",
    "learning_rate = 1e-4\n",
    "dim_dict = {'CLEVR': 512,\n",
    "            'gqa': 2048}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla K40m\n"
     ]
    }
   ],
   "source": [
    "print( torch.cuda.device_count() )\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print( torch.cuda.get_device_name(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataset_type, dataset_object):\n",
    "    \n",
    "#     # loading dataset using hdf5 imposing minimal overhead\n",
    "#     if dataset_type == \"CLEVR\":\n",
    "#         dataset_object = CLEVR('data/CLEVR_v1.0', transform=transform)\n",
    "#     else:\n",
    "#         dataset_object = GQA('data/gqa', transform=transform)\n",
    "\n",
    "    train_set = DataLoader(\n",
    "        dataset_object, batch_size=batch_size, num_workers=1, collate_fn=collate_data\n",
    "    )\n",
    "\n",
    "    dataset = iter(train_set)\n",
    "    pbar = tqdm(dataset)\n",
    "    moving_loss = 0\n",
    "\n",
    "    net.train(True)\n",
    "    for iter_id, (image, question, q_len, answer) in enumerate(pbar):\n",
    "        image, question, answer = (\n",
    "            image.to(device),\n",
    "            question.to(device),\n",
    "            answer.to(device),\n",
    "        )\n",
    "\n",
    "        net.zero_grad()\n",
    "        output = net(image, question, q_len)\n",
    "        loss = criterion(output, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct = output.detach().argmax(1) == answer\n",
    "        correct = torch.tensor(correct, dtype=torch.float32).sum() / batch_size\n",
    "\n",
    "        if moving_loss == 0:\n",
    "            moving_loss = correct\n",
    "        else:\n",
    "            moving_loss = (moving_loss * iter_id + correct) / (iter_id + 1)\n",
    "#             moving_loss = moving_loss * 0.99 + correct * 0.01 ??? swd\n",
    "\n",
    "        pbar.set_description('Epoch: {}; CurLoss: {:.5f}; CurAcc: {:.5f}; Tot_Acc: {:.5f}'.format(epoch + 1, loss.item(), correct, moving_loss))\n",
    "\n",
    "        accumulate(net_running, net, decay)\n",
    "\n",
    "#     dataset_object.close()\n",
    "\n",
    "\n",
    "def valid(epoch, dataset_type, dataset_object):\n",
    "    \n",
    "#     # loading dataset using hdf5 imposing minimal overhead\n",
    "#     if dataset_type == \"CLEVR\":\n",
    "#         dataset_object = CLEVR('data/CLEVR_v1.0', 'val', transform=None)\n",
    "#     else:\n",
    "#         dataset_object = GQA('data/gqa', 'val', transform=None)\n",
    "\n",
    "    valid_set = DataLoader(\n",
    "        dataset_object, batch_size=4*batch_size, num_workers=1, collate_fn=collate_data\n",
    "    )\n",
    "    dataset = iter(valid_set)\n",
    "\n",
    "    net_running.train(False)\n",
    "    correct_counts = 0\n",
    "    total_counts = 0\n",
    "    running_loss = 0.0\n",
    "    batches_done = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataset)\n",
    "        for image, question, q_len, answer in pbar:\n",
    "            image, question, answer = (\n",
    "                image.to(device),\n",
    "                question.to(device),\n",
    "                answer.to(device),\n",
    "            )\n",
    "\n",
    "            output = net_running(image, question, q_len)\n",
    "            loss = criterion(output, answer)\n",
    "            correct = output.detach().argmax(1) == answer\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            batches_done += 1\n",
    "            for c in correct:\n",
    "                if c:\n",
    "                    correct_counts += 1\n",
    "                total_counts += 1\n",
    "\n",
    "            pbar.set_description('Epoch: {}; Loss: {:.5f}; Acc: {:.5f}'.format(epoch + 1, loss.item(), correct_counts / total_counts))\n",
    "\n",
    "#     with open('log/log_{}.txt'.format(str(epoch + 1).zfill(2)), 'w') as w:\n",
    "#         w.write('{:.5f}\\n'.format(correct_counts / total_counts))\n",
    "## NO need to write the log here...\n",
    "    \n",
    "    val_acc = correct_counts / total_counts\n",
    "    val_loss = running_loss / total_counts\n",
    "    print('Validation Accuracy: {:.5f}'.format(val_acc))\n",
    "    print('Validation Loss: {:.8f}'.format(val_loss))\n",
    "    \n",
    "#     dataset_object.close()\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving result to:  result/try/\n"
     ]
    }
   ],
   "source": [
    "dataset_type = 'CLEVR'\n",
    "# input\n",
    "# args = parse_args()\n",
    "decay = 0.999\n",
    "load_embd = False\n",
    "out_name = 'try'\n",
    "\n",
    "out_directory = 'result/'+ out_name +'/'\n",
    "if not os.path.exists(out_directory):\n",
    "    os.makedirs(out_directory)\n",
    "print('Saving result to: ', out_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word embeddings from scratch...\n"
     ]
    }
   ],
   "source": [
    "if not load_embd:\n",
    "    with open(f'data/{dataset_type}_dic.pkl', 'rb') as f:\n",
    "        dic = pickle.load(f)\n",
    "    n_words = len(dic['word_dic']) + 1\n",
    "    n_answers = len(dic['answer_dic'])\n",
    "    print('Training word embeddings from scratch...')\n",
    "else:\n",
    "    # add codes for loading GLOVE, embd dimensions, and out dim\n",
    "    print('Loading GLOVE word embeddings...')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded in 2.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# loading dataset using hdf5 imposing minimal overhead\n",
    "since = time.time()\n",
    "if dataset_type == \"CLEVR\":\n",
    "    train_object = CLEVR('data/CLEVR_v1.0', transform=transform)\n",
    "    val_object = CLEVR('data/CLEVR_v1.0', 'val', transform=None)\n",
    "else:\n",
    "    train_object = GQA('data/gqa', transform=transform)\n",
    "    val_object = GQA('data/gqa', 'val', transform=None)\n",
    "print('Dataset loaded in %.2f seconds' %(time.time()-since) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MACNetwork(n_words, dim_dict[dataset_type], classes=n_answers, max_step=4)\n",
    "# net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "\n",
    "net_running = MACNetwork(n_words, dim_dict[dataset_type], classes=n_answers, max_step=4)\n",
    "# net_running = nn.DataParallel(net_running)\n",
    "net_running.to(device)\n",
    "\n",
    "accumulate(net_running, net, 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging training information\n",
    "with open(out_directory + 'log.txt', 'w') as outfile:\n",
    "    outfile.write('==== Training details ====\\n')\n",
    "    outfile.write('---- Model structure ----\\n')\n",
    "    outfile.write('Loading GLOVE embedding:  %s.     dictionary dim: %d. \\n' %(load_embd, n_words))\n",
    "    outfile.write('Hidden dimension: %d.     Output dimension: %d.\\n' %(dim_dict[dataset_type], n_answers))\n",
    "\n",
    "    outfile.write('\\n---- Training detials ----\\n')\n",
    "    outfile.write('Batch size:  %d.     RA_decay: %f\\n' %(batch_size, decay))\n",
    "    outfile.write('Learning rate: %f.     Epochs: %d\\n' %(learning_rate, n_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4daeb19d36b043f9987a5fd76dfa18e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/tianwei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4086aed1adb436f9168c1492cb7d657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.42486\n",
      "Validation Loss: 0.00296304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15026231550b4f9d858224e0d3033dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.42486\n",
      "Validation Loss: 0.00296304\n",
      "Accuracy increased from 0.4249 to 0.0000, saved to result/try/checkpoint.model. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8954eeeebc9d40b5a168c00c61584758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d9969abc1e4500b6fa31b4e376e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.50114\n",
      "Validation Loss: 0.00188413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16d9adc7ce744819e205e04b6072302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Accuracy: 0.50114\n",
      "Validation Loss: 0.00188413\n",
      "Accuracy increased from 0.5011 to 0.4249, saved to result/try/checkpoint.model. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783f330d79f4405abe620007a7fbb7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae1323bf3259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-799d7747aa89>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, dataset_type, dataset_object)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmoving_loss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_curve = np.zeros([0,4])\n",
    "acc_best = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train(epoch, dataset_type, val_object)\n",
    "    train_acc, train_loss = valid(epoch, dataset_type, val_object)\n",
    "    val_acc, val_loss = valid(epoch, dataset_type, val_object)\n",
    "\n",
    "    # saving training result details.\n",
    "    learning_curve = np.append(learning_curve, np.array([[train_acc,val_acc,train_loss,val_loss]]), axis = 0)\n",
    "    savetxt(out_directory+'learn_curve.csv', learning_curve, delimiter=',')\n",
    "\n",
    "    # saving trained models\n",
    "    if val_acc > acc_best:\n",
    "        with open(out_directory+'checkpoint.model', 'wb') as f:\n",
    "    #         with open('checkpoint/checkpoint_{}.model'.format(str(epoch + 1).zfill(2)), 'wb') as f:\n",
    "            torch.save(net_running.state_dict(), f)\n",
    "        print('Accuracy increased to %.4f from %.4f, saved to %s. '%(val_acc, acc_best, out_directory+'checkpoint.model'))\n",
    "        acc_best = val_acc\n",
    "\n",
    "print('The best validation accuracy: ', acc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training time control:\n",
    "\n",
    "### using a single K80 GPU, around 1h10min?\n",
    "\n",
    "### using two K80 GPU:  batch size problem with LSTM.... some internal problem with torch\n",
    "\n",
    "### using a single K40 GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train for val set: 14-15min.\n",
    "## val for val set: 7-8min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
