{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import CLEVR, collate_data, transform, GQA\n",
    "from src.model_predu import MACNetwork\n",
    "# from model_gqa import MACNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla K40m\n"
     ]
    }
   ],
   "source": [
    "print( torch.cuda.device_count() )\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print( torch.cuda.get_device_name(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "dim_dict = {'CLEVR': 512,\n",
    "            'gqa': 2048}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params updating using running average \n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "        \n",
    "        \n",
    "def train(epoch, dataset_type, dataset_object):\n",
    "\n",
    "    train_set = DataLoader(\n",
    "        dataset_object, batch_size=batch_size, num_workers=1, collate_fn=collate_data\n",
    "#         dataset_object, batch_size=batch_size, num_workers=multiprocessing.cpu_count(), collate_fn=collate_data\n",
    "    )\n",
    "\n",
    "    dataset = iter(train_set)\n",
    "    pbar = tqdm(dataset)\n",
    "    moving_loss = 0\n",
    "    \n",
    "    log_softmax = nn.LogSoftmax().cuda()\n",
    "    aleatoric_loss = AleatoricCrossEntropyLoss().cuda()\n",
    "\n",
    "    net.train(True)\n",
    "    for iter_id, (image, question, q_len, answer) in enumerate(pbar):\n",
    "        image, question, answer = (\n",
    "            image.to(device),\n",
    "            question.to(device),\n",
    "            answer.to(device),\n",
    "        )\n",
    "        y = one_hot_embedding(answer, 28)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, logits_variance = net(image, question, q_len)\n",
    "        \n",
    "        nll = -log_softmax(output)\n",
    "        ud_ce_loss = (nll * y / 10).sum(dim=1).mean()\n",
    "        \n",
    "        gce_loss, variance_loss, undistorted_loss, variance_depressor = aleatoric_loss(logits_variance, output, y)\n",
    "        aleatoric_uncertainty_loss = gce_loss + variance_loss + variance_depressor\n",
    "        loss = ud_ce_loss + aleatoric_uncertainty_loss\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        aleatoric_uncertainty_loss.backward(retain_graph=True)  \n",
    "        # in multi-task learning scenario, have to choose \"retain_graph=True\" before performing multiple \"backward\"\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        correct = output.detach().argmax(1) == answer\n",
    "        correct = torch.tensor(correct, dtype=torch.float32).sum() / batch_size\n",
    "\n",
    "        if moving_loss == 0:\n",
    "            moving_loss = correct\n",
    "        else:\n",
    "            moving_loss = (moving_loss * iter_id + correct) / (iter_id + 1)\n",
    "\n",
    "        pbar.set_description('Epoch: {}; CurLoss: {:.8f}; CurAcc: {:.5f}; Tot_Acc: {:.5f}'.format(epoch + 1, loss.item(), correct, moving_loss))\n",
    "\n",
    "        accumulate(net_running, net, decay)\n",
    "#     dataset_object.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def valid(epoch, dataset_type, dataset_object):\n",
    "\n",
    "    valid_set = DataLoader(\n",
    "        dataset_object, batch_size=4*batch_size, num_workers=1,  collate_fn=collate_data\n",
    "#         dataset_object, batch_size=4*batch_size, num_workers=multiprocessing.cpu_count(), collate_fn=collate_data\n",
    "    )\n",
    "    dataset = iter(valid_set)\n",
    "\n",
    "    net_running.train(False)\n",
    "    correct_counts = 0\n",
    "    total_counts = 0\n",
    "    running_loss = 0.0\n",
    "    batches_done = 0\n",
    "    \n",
    "    log_softmax = nn.LogSoftmax().cuda()\n",
    "    aleatoric_loss = AleatoricCrossEntropyLoss().cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataset)\n",
    "        for image, question, q_len, answer in pbar:\n",
    "            image, question, answer = (\n",
    "                image.to(device),\n",
    "                question.to(device),\n",
    "                answer.to(device),\n",
    "            )\n",
    "\n",
    "            y = one_hot_embedding(answer, 28)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output, logits_variance = net(image, question, q_len)\n",
    "\n",
    "            nll = -log_softmax(output)\n",
    "            ud_ce_loss = (nll * y / 10).sum(dim=1).mean()\n",
    "\n",
    "            gce_loss, variance_loss, undistorted_loss, variance_depressor = aleatoric_loss(logits_variance, output, y)\n",
    "            aleatoric_uncertainty_loss = gce_loss + variance_loss + variance_depressor\n",
    "            loss = ud_ce_loss + aleatoric_uncertainty_loss\n",
    "            \n",
    "            correct = output.detach().argmax(1) == answer\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            batches_done += 1\n",
    "            for c in correct:\n",
    "                if c:\n",
    "                    correct_counts += 1\n",
    "                total_counts += 1\n",
    "\n",
    "            pbar.set_description('Epoch: {}; Loss: {:.8f}; Acc: {:.5f}'.format(epoch + 1, loss.item(), correct_counts / total_counts))\n",
    "\n",
    "    val_acc = correct_counts / total_counts\n",
    "    val_loss = running_loss / total_counts\n",
    "    print('Validation Accuracy: {:.5f}'.format(val_acc))\n",
    "    print('Validation Loss: {:.8f}'.format(val_loss))\n",
    "    \n",
    "#     dataset_object.close()\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aleatoric uncertainty\n",
    "def one_hot_embedding(labels, num_classes=10):\n",
    "    # Convert to One Hot Encoding\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "\n",
    "\n",
    "class AleatoricCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AleatoricCrossEntropyLoss, self).__init__()\n",
    "        self.elu = nn.ELU()\n",
    "        self.categorical_crossentropy = undistorted_cross_entropy()\n",
    "        # swd: \n",
    "        self.T = 100  # monte carlo number\n",
    "        self.num_classes = 28\n",
    "        \n",
    "\n",
    "    def forward(self, logit_var, pred, true):\n",
    "        std = torch.sqrt(logit_var)\n",
    "        variance = logit_var\n",
    "\n",
    "        variance_diff = torch.exp(variance) - torch.ones_like(variance)\n",
    "        variance_depressor = torch.mean(variance_diff)\n",
    "        undistorted_loss = self.categorical_crossentropy(pred, true)\n",
    "\n",
    "        dist = torch.distributions.Normal(torch.zeros_like(std), std)\n",
    "\n",
    "        monte_carlo_results = 0\n",
    "        monte_carlo_results_gce = 0\n",
    "        for i in range(0, self.T):\n",
    "            mc_gce_loss, mc_gce_diff_loss = self.gaussian_categorical_crossentropy(pred, true, dist, undistorted_loss)\n",
    "            monte_carlo_results = monte_carlo_results + mc_gce_diff_loss\n",
    "            monte_carlo_results_gce = monte_carlo_results_gce + mc_gce_loss\n",
    "        variance_loss = (monte_carlo_results / self.T)\n",
    "        gce_loss = (monte_carlo_results_gce / self.T)\n",
    "\n",
    "        return gce_loss, variance_loss, undistorted_loss, variance_depressor\n",
    "\n",
    "    def gaussian_categorical_crossentropy(self, pred, true, dist, undistorted_loss):\n",
    "        std_samples = (dist.sample_n(self.num_classes)).transpose(0, 1)\n",
    "        std_samples = std_samples.view(std_samples.size(0), -1)\n",
    "\n",
    "        distorted_loss = self.categorical_crossentropy(pred + std_samples, true)\n",
    "        diff = undistorted_loss - distorted_loss\n",
    "\n",
    "        return distorted_loss, -self.elu(diff)\n",
    "\n",
    "    def categorical_cross_entropy(true, pred):\n",
    "        return np.sum(true * np.log(pred), axis=1)\n",
    "\n",
    "\n",
    "class undistorted_cross_entropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(undistorted_cross_entropy, self).__init__()\n",
    "        self.log_softmax = nn.LogSoftmax().cuda()\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        nll = -self.log_softmax(pred)\n",
    "        return (nll * true / 10).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving result to:  result/try/\n",
      "Training word embeddings from scratch...\n",
      "Dataset loaded in 2.02 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset_type = 'CLEVR'\n",
    "# input\n",
    "decay = 0.999\n",
    "load_embd = False\n",
    "out_name = 'try'\n",
    "n_epoch = 25\n",
    "\n",
    "out_directory = 'result/'+ out_name +'/'\n",
    "if not os.path.exists(out_directory):\n",
    "    os.makedirs(out_directory)\n",
    "print('Saving result to: ', out_directory)\n",
    "\n",
    "if not load_embd:\n",
    "    with open(f'data/{dataset_type}_dic.pkl', 'rb') as f:\n",
    "        dic = pickle.load(f)\n",
    "    n_words = len(dic['word_dic']) + 1\n",
    "    n_answers = len(dic['answer_dic'])\n",
    "    print('Training word embeddings from scratch...')\n",
    "else:\n",
    "    # add codes for loading GLOVE, embd dimensions, and out dim\n",
    "    print('Loading GLOVE word embeddings...')\n",
    "    pass\n",
    "\n",
    "\n",
    "# loading dataset using hdf5 imposing minimal overhead\n",
    "since = time.time()\n",
    "if dataset_type == \"CLEVR\":\n",
    "    train_object = CLEVR('data/CLEVR_v1.0', transform=transform)\n",
    "    val_object = CLEVR('data/CLEVR_v1.0', 'val', transform=None)\n",
    "else:\n",
    "    train_object = GQA('data/gqa', transform=transform)\n",
    "    val_object = GQA('data/gqa', 'val', transform=None)\n",
    "print('Dataset loaded in %.2f seconds' %(time.time()-since) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MACNetwork(n_words, dim_dict[dataset_type], classes=n_answers, max_step=4).to(device)\n",
    "net_running = MACNetwork(n_words, dim_dict[dataset_type], classes=n_answers, max_step=4).to(device)\n",
    "accumulate(net_running, net, 0)\n",
    "\n",
    "# criterion/loss defined in train/valid\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging training information\n",
    "with open(out_directory + 'log.txt', 'w') as outfile:\n",
    "    outfile.write('==== Training details ====\\n')\n",
    "    outfile.write('---- Model structure ----\\n')\n",
    "    outfile.write('Loading GLOVE embedding:  %s.     dictionary dim: %d. \\n' %(load_embd, n_words))\n",
    "    outfile.write('Hidden dimension: %d.     Output dimension: %d.\\n' %(dim_dict[dataset_type], n_answers))\n",
    "\n",
    "    outfile.write('\\n---- Training detials ----\\n')\n",
    "    outfile.write('Batch size:  %d.     RA_decay: %f\\n' %(batch_size, decay))\n",
    "    outfile.write('Learning rate: %f.     Epochs: %d\\n' %(learning_rate, n_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5469 [00:00<?, ?it/s]/u/tianwei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/u/tianwei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/u/tianwei/anaconda3/lib/python3.7/site-packages/torch/distributions/distribution.py:134: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n",
      "  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n",
      "/u/tianwei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Epoch: 1; CurLoss: 0.56503868; CurAcc: 0.20312; Tot_Acc: 0.14752:   0%|          | 17/5469 [00:17<1:29:16,  1.02it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-44bdf910b8b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5920fefe196c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, dataset_type, dataset_object)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmoving_loss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_curve = np.zeros([0,4])\n",
    "acc_best = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train(epoch, dataset_type, train_object)\n",
    "    train_acc, train_loss = valid(epoch, dataset_type, train_object)\n",
    "    val_acc, val_loss = valid(epoch, dataset_type, val_object)\n",
    "\n",
    "    # saving training result details.\n",
    "    learning_curve = np.append(learning_curve, np.array([[train_acc,val_acc,train_loss,val_loss]]), axis = 0)\n",
    "    savetxt(out_directory+'learn_curve.csv', learning_curve, delimiter=',')\n",
    "\n",
    "    # saving trained models\n",
    "    if val_acc > acc_best:\n",
    "        with open(out_directory+'checkpoint.model', 'wb') as f:\n",
    "#         with open('checkpoint/checkpoint_{}.model'.format(str(epoch + 1).zfill(2)), 'wb') as f:\n",
    "            torch.save(net_running.state_dict(), f)\n",
    "        print('Accuracy increased from %.4f to %.4f, saved to %s. '%(acc_best, val_acc, out_directory+'checkpoint.model'))\n",
    "        acc_best = val_acc\n",
    "\n",
    "print('The best validation accuracy: ', acc_best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
