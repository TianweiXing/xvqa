{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import CLEVR, collate_data, transform, GQA\n",
    "from src.edl_utils import *\n",
    "from src.model import MACNetwork\n",
    "# from model_gqa import MACNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "dim_dict = {'CLEVR': 512,\n",
    "            'gqa': 2048}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Tesla K40m\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print( torch.cuda.device_count() )\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print( torch.cuda.get_device_name(i) )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params updating using running average \n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(1 - decay, par2[k].data)\n",
    "\n",
    "\n",
    "        \n",
    "def train(epoch, dataset_type, dataset_object):\n",
    "\n",
    "    train_set = DataLoader(\n",
    "        dataset_object, batch_size=batch_size, num_workers=1, collate_fn=collate_data\n",
    "#         dataset_object, batch_size=batch_size, num_workers=multiprocessing.cpu_count(), collate_fn=collate_data\n",
    "    )\n",
    "    \n",
    "    phase = 'train'  # swd\n",
    "    uncertainty = True\n",
    "    num_classes = 28 # 28 classes for CLEVR!\n",
    "\n",
    "    dataset = iter(train_set)\n",
    "    pbar = tqdm(dataset)\n",
    "    moving_loss = 0\n",
    "\n",
    "    net.train(True)\n",
    "    for iter_id, (image, question, q_len, answer) in enumerate(pbar):\n",
    "        image, question, answer = (\n",
    "            image.to(device),\n",
    "            question.to(device),\n",
    "            answer.to(device),\n",
    "        )\n",
    "\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == \"train\"):\n",
    "            if uncertainty:\n",
    "                y = one_hot_embedding(answer, num_classes)\n",
    "                y = y.to(device)\n",
    "                outputs = net(image, question, q_len)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(\n",
    "                    outputs, y.float(), epoch, num_classes, 10, device)  # 10 is the annealing step\n",
    "\n",
    "                match = torch.reshape(torch.eq(\n",
    "                    preds, answer).float(), (-1, 1))\n",
    "                acc = torch.mean(match)\n",
    "                evidence = relu_evidence(outputs)\n",
    "                alpha = evidence + 1\n",
    "                u = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "\n",
    "                total_evidence = torch.sum(evidence, 1, keepdim=True)\n",
    "                mean_evidence = torch.mean(total_evidence)\n",
    "                mean_evidence_succ = torch.sum(\n",
    "                    torch.sum(evidence, 1, keepdim=True) * match) / torch.sum(match + 1e-20)\n",
    "                mean_evidence_fail = torch.sum(\n",
    "                    torch.sum(evidence, 1, keepdim=True) * (1 - match)) / (torch.sum(torch.abs(1 - match)) + 1e-20)\n",
    "\n",
    "            else:\n",
    "                outputs = net(image, question, q_len)\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, answer)\n",
    "\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        correct = outputs.detach().argmax(1) == answer\n",
    "        correct = torch.tensor(correct, dtype=torch.float32).sum() / batch_size\n",
    "\n",
    "        if moving_loss == 0:\n",
    "            moving_loss = correct\n",
    "        else:\n",
    "            moving_loss = (moving_loss * iter_id + correct) / (iter_id + 1)\n",
    "\n",
    "        pbar.set_description('Epoch: {}; CurLoss: {:.8f}; CurAcc: {:.5f}; Tot_Acc: {:.5f}'.format(epoch + 1, loss.item(), correct, moving_loss))\n",
    "\n",
    "        accumulate(net_running, net, decay)\n",
    "    return\n",
    "\n",
    "\n",
    "def valid(epoch, dataset_type, dataset_object):\n",
    "\n",
    "    valid_set = DataLoader(\n",
    "        dataset_object, batch_size=4*batch_size, num_workers=1, collate_fn=collate_data\n",
    "#         dataset_object, batch_size=4*batch_size, num_workers=multiprocessing.cpu_count(), collate_fn=collate_data\n",
    "    )\n",
    "    dataset = iter(valid_set)\n",
    "    \n",
    "    uncertainty = True\n",
    "    num_classes = 28\n",
    "    \n",
    "    net_running.train(False)\n",
    "    correct_counts = 0\n",
    "    total_counts = 0\n",
    "    running_loss = 0.0\n",
    "    batches_done = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataset)\n",
    "        for image, question, q_len, answer in pbar:\n",
    "            image, question, answer = (\n",
    "                image.to(device),\n",
    "                question.to(device),\n",
    "                answer.to(device),\n",
    "            )\n",
    "\n",
    "#             output = net_running(image, question, q_len)\n",
    "#             loss = criterion(output, answer)\n",
    "            \n",
    "            y = one_hot_embedding(answer, num_classes)\n",
    "            y = y.to(device)\n",
    "            outputs = net_running(image, question, q_len)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(\n",
    "                outputs, y.float(), epoch, num_classes, 10, device)  # 10 is the annealing step\n",
    "            \n",
    "            correct = outputs.detach().argmax(1) == answer\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            batches_done += 1\n",
    "            for c in correct:\n",
    "                if c:\n",
    "                    correct_counts += 1\n",
    "                total_counts += 1\n",
    "\n",
    "            pbar.set_description('Epoch: {}; Loss: {:.5f}; Acc: {:.5f}'.format(epoch + 1, loss.item(), correct_counts / total_counts))\n",
    "\n",
    "\n",
    "    val_acc = correct_counts / total_counts\n",
    "    val_loss = running_loss / total_counts\n",
    "    print('Validation Accuracy: {:.5f}'.format(val_acc))\n",
    "    print('Validation Loss: {:.8f}'.format(val_loss))\n",
    "    \n",
    "#     dataset_object.close()\n",
    "    return val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving result to:  result/try/\n",
      "Training word embeddings from scratch...\n"
     ]
    }
   ],
   "source": [
    "dataset_type = 'CLEVR'\n",
    "# input\n",
    "decay = 0.999\n",
    "load_embd = False\n",
    "out_name = 'try'\n",
    "n_epoch = 25\n",
    "\n",
    "out_directory = 'result/'+ out_name +'/'\n",
    "if not os.path.exists(out_directory):\n",
    "    os.makedirs(out_directory)\n",
    "print('Saving result to: ', out_directory)\n",
    "\n",
    "if not load_embd:\n",
    "    with open(f'data/{dataset_type}_dic.pkl', 'rb') as f:\n",
    "        dic = pickle.load(f)\n",
    "    n_words = len(dic['word_dic']) + 1\n",
    "    n_answers = len(dic['answer_dic'])\n",
    "    print('Training word embeddings from scratch...')\n",
    "else:\n",
    "    # add codes for loading GLOVE, embd dimensions, and out dim\n",
    "    print('Loading GLOVE word embeddings...')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded in 1.97 seconds\n"
     ]
    }
   ],
   "source": [
    "# loading dataset using hdf5 imposing minimal overhead\n",
    "since = time.time()\n",
    "if dataset_type == \"CLEVR\":\n",
    "    train_object = CLEVR('data/CLEVR_v1.0', transform=transform)\n",
    "    val_object = CLEVR('data/CLEVR_v1.0', 'val', transform=None)\n",
    "else:\n",
    "    train_object = GQA('data/gqa', transform=transform)\n",
    "    val_object = GQA('data/gqa', 'val', transform=None)\n",
    "print('Dataset loaded in %.2f seconds' %(time.time()-since) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MACNetwork(n_words, dim_dict[dataset_type], classes=n_answers, max_step=4).to(device)\n",
    "net_running = MACNetwork(n_words, dim_dict[dataset_type], classes=n_answers, max_step=4).to(device)\n",
    "accumulate(net_running, net, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edl training\n",
    "use_uncertainty = True\n",
    "loss_func = 'digamma'\n",
    "\n",
    "import torch.optim as optim\n",
    "if use_uncertainty:\n",
    "    if loss_func == 'digamma':\n",
    "        criterion = edl_digamma_loss\n",
    "    elif loss_func == 'log':\n",
    "        criterion = edl_log_loss\n",
    "    elif loss_func == 'mse':\n",
    "        criterion = edl_mse_loss\n",
    "    else:\n",
    "        parser.error(\"--uncertainty requires --mse, --log or --digamma.\")\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.005)   \n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging training information\n",
    "with open(out_directory + 'log.txt', 'w') as outfile:\n",
    "    outfile.write('==== Training details ====\\n')\n",
    "    outfile.write('---- Model structure ----\\n')\n",
    "    outfile.write('Loading GLOVE embedding:  %s.     dictionary dim: %d. \\n' %(load_embd, n_words))\n",
    "    outfile.write('Hidden dimension: %d.     Output dimension: %d.\\n' %(dim_dict[dataset_type], n_answers))\n",
    "\n",
    "    outfile.write('\\n---- Training detials ----\\n')\n",
    "    outfile.write('Batch size:  %d.     RA_decay: %f\\n' %(batch_size, decay))\n",
    "    outfile.write('Learning rate: %f.     Epochs: %d\\n' %(learning_rate, n_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5469 [00:00<?, ?it/s]\u001b[A/u/tianwei/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "Epoch: 1; CurLoss: 2.87849569; CurAcc: 0.14844; Tot_Acc: 0.14844:   0%|          | 0/5469 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.87849569; CurAcc: 0.14844; Tot_Acc: 0.14844:   0%|          | 1/5469 [00:00<1:20:31,  1.13it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.76361370; CurAcc: 0.17188; Tot_Acc: 0.16016:   0%|          | 1/5469 [00:01<1:20:31,  1.13it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.76361370; CurAcc: 0.17188; Tot_Acc: 0.16016:   0%|          | 2/5469 [00:01<1:13:40,  1.24it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.87284565; CurAcc: 0.21875; Tot_Acc: 0.17969:   0%|          | 2/5469 [00:02<1:13:40,  1.24it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.87284565; CurAcc: 0.21875; Tot_Acc: 0.17969:   0%|          | 3/5469 [00:02<1:08:53,  1.32it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.91383147; CurAcc: 0.20312; Tot_Acc: 0.18555:   0%|          | 3/5469 [00:02<1:08:53,  1.32it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.91383147; CurAcc: 0.20312; Tot_Acc: 0.18555:   0%|          | 4/5469 [00:02<1:05:38,  1.39it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.85994911; CurAcc: 0.17969; Tot_Acc: 0.18438:   0%|          | 4/5469 [00:03<1:05:38,  1.39it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.85994911; CurAcc: 0.17969; Tot_Acc: 0.18438:   0%|          | 5/5469 [00:03<1:03:16,  1.44it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 3.00492573; CurAcc: 0.17969; Tot_Acc: 0.18359:   0%|          | 5/5469 [00:04<1:03:16,  1.44it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 3.00492573; CurAcc: 0.17969; Tot_Acc: 0.18359:   0%|          | 6/5469 [00:04<1:01:33,  1.48it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.63741469; CurAcc: 0.24219; Tot_Acc: 0.19196:   0%|          | 6/5469 [00:04<1:01:33,  1.48it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.63741469; CurAcc: 0.24219; Tot_Acc: 0.19196:   0%|          | 7/5469 [00:04<1:00:05,  1.51it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.79263687; CurAcc: 0.20312; Tot_Acc: 0.19336:   0%|          | 7/5469 [00:05<1:00:05,  1.51it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.79263687; CurAcc: 0.20312; Tot_Acc: 0.19336:   0%|          | 8/5469 [00:05<59:32,  1.53it/s]  \u001b[A\n",
      "Epoch: 1; CurLoss: 2.98706484; CurAcc: 0.19531; Tot_Acc: 0.19358:   0%|          | 8/5469 [00:05<59:32,  1.53it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.98706484; CurAcc: 0.19531; Tot_Acc: 0.19358:   0%|          | 9/5469 [00:05<59:07,  1.54it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.99744821; CurAcc: 0.17188; Tot_Acc: 0.19141:   0%|          | 9/5469 [00:06<59:07,  1.54it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.99744821; CurAcc: 0.17188; Tot_Acc: 0.19141:   0%|          | 10/5469 [00:06<59:06,  1.54it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.89325643; CurAcc: 0.19531; Tot_Acc: 0.19176:   0%|          | 10/5469 [00:07<59:06,  1.54it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.89325643; CurAcc: 0.19531; Tot_Acc: 0.19176:   0%|          | 11/5469 [00:07<58:51,  1.55it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.90682554; CurAcc: 0.17969; Tot_Acc: 0.19076:   0%|          | 11/5469 [00:07<58:51,  1.55it/s]\u001b[A\n",
      "Epoch: 1; CurLoss: 2.90682554; CurAcc: 0.17969; Tot_Acc: 0.19076:   0%|          | 12/5469 [00:07<58:13,  1.56it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "learning_curve = np.zeros([0,4])\n",
    "acc_best = 0\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train(epoch, dataset_type, train_object)\n",
    "    train_acc, train_loss = valid(epoch, dataset_type, train_object)\n",
    "    val_acc, val_loss = valid(epoch, dataset_type, val_object)\n",
    "\n",
    "    # saving training result details.\n",
    "    learning_curve = np.append(learning_curve, np.array([[train_acc,val_acc,train_loss,val_loss]]), axis = 0)\n",
    "    savetxt(out_directory+'learn_curve.csv', learning_curve, delimiter=',')\n",
    "\n",
    "    # saving trained models\n",
    "    if val_acc > acc_best:\n",
    "        with open(out_directory+'checkpoint.model', 'wb') as f:\n",
    "#         with open('checkpoint/checkpoint_{}.model'.format(str(epoch + 1).zfill(2)), 'wb') as f:\n",
    "            torch.save(net_running.state_dict(), f)\n",
    "        print('Accuracy increased from %.4f to %.4f, saved to %s. '%(acc_best, val_acc, out_directory+'checkpoint.model'))\n",
    "        acc_best = val_acc\n",
    "\n",
    "print('The best validation accuracy: ', acc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
